{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever wondered how we process and learn information? For example, how does our body process information such that we are able to move our hands or legs? To put it simply, the brain will process information and then send out signals to the rest of the body to trigger certain muscle movements. These signals are transported through the nervous system. One of the main components of the nervous system are neuron cells. These cells work on a threshold basis which means that the signal will only be transferred from cells to cells if it is higher than a certain value or amount. As such, when we decide to move our hands, the signals from the brain will get transferred to the muscle in our hands and not the muscle in our legs.\n",
    "\n",
    "### Training Artificial Neural Networks\n",
    "However, this only explains how we process information. How about the ability of humans to learn? For example, why do we know to stop at a red light or how to kick a ball? It is because we were trained to do so by looking at examples or how other people were doing it. Through these examples, we were able to learn and remember.\n",
    "\n",
    "Would it be great if computers were able to mimic the way humans process and learn information? With artificial neural network, this can be done! Artificial neural networks are able to process and 'learn' complex relationships within datasets. An illustration of a simple neural network is shown below.\n",
    "\n",
    "The basic idea is that we input data into the input layer. Data will be processed in the subsequent hidden layer(s) - we only show one hidden layer on the picture below, but it can be many layers. Each layer is made of multiple artificial neurons which apply functions to the data and pass it on to another hidden layer, finally ending with the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/ANN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustration above shows a simple neural network with 1 input layer, 1 hidden layer (in between an input layer and an output layer) and 1 output layer. Each circle represent 1 node or 1 neuron. We usually do not discuss the number of nodes at the input layer within the model architecture as the input layer is just the data that is being passed to the model. Thus, the hidden layer has 4 nodes/neurons and the output layer has 1 node/neuron. \n",
    "\n",
    "The output layer will show the results of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the neural network work? How can they be useful for machine learning project? Watch this [video](https://www.youtube.com/watch?v=aircAruvnKk) to find out more about artificial neural networks. Pause the video and take time to try and understand how the neural network works. Note down any interesting information on neural networks on your worksheet. Are you also able to draw out the network (similar to the illustration above) if the network has 1 input layer with 5 nodes, 2 hidden layers with 3 nodes each and 1 output layer with 2 nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks can be used to do regression and classification.\n",
    "# Neural networks hyperparamter tuning.\n",
    "# Can't drieve how a neural network get to it's results unlike traditional\n",
    "# machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the different features of an artificial neural network, one question that still remains is how does the network \"learn\"?\n",
    "\n",
    "Take for example a young basketball player who is learning to shoot a 3-point shot. If he shoots and he misses because the shot is too short, the basketball player will adjust and increase the strength of the next shot. If the next shot now is too far right of the basket, the player will again adjust his shot to shoot more towards the center. The player continues to do this until the shot is made. The player then remembers the exact strength and shot direction when shooting 3-point shots in future.\n",
    "\n",
    "This is similar to how neural networks are trained. First, the data is passed through the network and a predicted output is given. This is known as a forward propagation. The predicted output is then compared to the actual output of the data and the differences between the predicted and actual will be passed backwards through the model. During the backward pass, adjustments will be made within the model such that the differences between prediction output and actual output will be reduced. This is known as the backpropagation. After the adjustments are made, data will be passed through from the input layer again and another predicted output will be made. The new predicted output will be compared to the actual output again and the differences will be passed backwards through the model. More adjustments will be made within the model.\n",
    "\n",
    "The process of forward propagation and backpropagation will be repeated until the differences between predicted output and the actual output are minimised. The model is now trained and can be used for prediction of other similar datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of another example in your life that is similar to the way the neural networks are trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making corrections for your exam paper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example to help you understand neural networks better. You have been tasked to bake a cake for your father's birthday. You follow the basic cookbook and come up with a basic cake. As you want your cake to be the best cake anyone has eaten, you approach your mother for a taste test. Your mother replies that the cake is too sweet and slightly burnt. Thus, you try and adjust the amount of sugar and also the time the cake spent in the oven. Next, you will then bake another cake with your new recipe and baking time before doing another taste test with your mother. This continues until you get the perfect cake.\n",
    "\n",
    "The first steps to bake the cake are similar to the forward propagation step within the artificial neural network. The taste test is similar to the comparison of a predicted output to the actual output. The adjustment of the amount of sugar and baking time is similar to backpropagation within the neural network. The repetition in the steps are similar to the full training process of the model.\n",
    "\n",
    "To have a better visualisation of how backpropagation works, watch this [video](https://www.youtube.com/watch?v=Ilg3gGewQ5U) and take down any information that interests you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bonus: You are not required to understand the actual adjustments within the model. However, if you are mathematically inclined or really interested in understanding all the adjustments and have time, you can watch the 2 videos listed below. Note down any interesting information in your worksheet or the cell below. </font>\n",
    "- [video 1](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [video 2](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "# Getting to know how Gradient descent works\n",
    "# Getting to know how backpropagation works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network with the Iris Flower dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the neural network using the Iris Flower dataset!\n",
    "\n",
    "## 1. Import required Libraries\n",
    "Firstly, we will import the required libraries: pandas and numpy to provide data structure, and scikit learn to gain access to artificial neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/PetalSepal1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain and explore dataset\n",
    "\n",
    "Import the Iris Flower dataset as a dataframe df below. \n",
    "The file should be iris.data. \n",
    "\n",
    "Remember to include the headers for all the columns. \n",
    "\n",
    "Refer to the picture (source: https://www.researchgate.net/figure/Trollius-ranunculoide-flower-with-measured-traits_fig6_272514310) above to understand the variables. \n",
    "\n",
    "We'll first explore the data. Do you remember how this is done?\n",
    "1. Open the csv file and put it into a dataframe.\n",
    "2. Include headers\n",
    "3. Use .info() and .describe() to see basic information about the dataset\n",
    "\n",
    "Check for any missing values or erroneous data. Are there any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "names = [\"sepal_length\", \"sepal_width\", 'petal_length', 'petal_width', 'class']\n",
    "path = 'data/iris.data'\n",
    "df = pd.read_csv(path, header=None, names=names)\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determine features and target values\n",
    "\n",
    "Now, we have to split the datset into the x values (features which the model can learn relationships from) and the y values (target values or expected output from the model). \n",
    "\n",
    "### Standardization\n",
    "We would also have to standardize the dataset. \n",
    "What is standardization for? To understand this, look at the distribution of data above! Notice the different mean and standard deviation. Comparing between these variables will be difficult. Standardization helps us to equalize these various distributions into a common mean and standart deviation, so that we can compare them easily. Refer to the [standardscaler graph here](http://benalexkeen.com/feature-scaling-with-scikit-learn/). See how data changed before and after scaling. \n",
    "\n",
    "\n",
    "This is to allow neural networks to classify easily. The code below will extract out the x values as x_values and also standardise the values. We will extract the y_values later. Run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "standardise = StandardScaler() # the standardscaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1\n",
    "x_values = standardise.fit_transform(x_values)\n",
    "x_values_df = pd.DataFrame(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this standardized dataset to the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.900681  1.032057 -1.341272 -1.312977\n",
      "1 -1.143017 -0.124958 -1.341272 -1.312977\n",
      "2 -1.385353  0.337848 -1.398138 -1.312977\n",
      "3 -1.506521  0.106445 -1.284407 -1.312977\n",
      "4 -1.021849  1.263460 -1.341272 -1.312977\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the .describe function to find out what is the current mean and std value of the standardized dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3\n",
      "count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02\n",
      "mean  -4.736952e-16 -6.631732e-16  3.315866e-16 -2.842171e-16\n",
      "std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00\n",
      "min   -1.870024e+00 -2.438987e+00 -1.568735e+00 -1.444450e+00\n",
      "25%   -9.006812e-01 -5.877635e-01 -1.227541e+00 -1.181504e+00\n",
      "50%   -5.250608e-02 -1.249576e-01  3.362659e-01  1.332259e-01\n",
      "75%    6.745011e-01  5.692513e-01  7.627586e-01  7.905908e-01\n",
      "max    2.492019e+00  3.114684e+00  1.786341e+00  1.710902e+00\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the neural network\n",
    "\n",
    "We can now build a simple neural network. In order to do so, we will need to import the dense and sequential functions from keras library. \n",
    "\n",
    "### Sequential\n",
    "The Sequential model allows you to first create an empty model object, and then add layers to it one after another in sequence.\n",
    "\n",
    "### Dense\n",
    "A dense layer is simply a layer of neurons in the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8f/54_36yvd3h7cthjr842rqlth0000gn/T/ipykernel_1262/926589766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a neural network with 1 input layer, 2 hidden layers and 1 output layer. There are no rules to decide how many nodes should be within the hidden layers. \n",
    "\n",
    "For this neural network, we will use 6 hidden nodes for each hidden layer. \n",
    "\n",
    "With regards to the output layer, we should use as many nodes as the number of classes. How many nodes should we use for the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you draw the neural network you are creating? Draw them now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # O  O\n",
    "# O   O  O  O\n",
    "# O   O  O  O\n",
    "# O   O  O  O\n",
    "# O   O  O\n",
    "    # O  O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the code below to build the artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the neural network as model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: [Comparisons between activation functions](http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/)\n",
    "You've learnt about ReLu earlier in Acquire-CV. There are many more activation functions. They are like [on-off buttons](https://en.wikipedia.org/wiki/Activation_function) to allow certain data/input to follow through the neurons or not. You are not expected to know the functions in detail for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out the model summary after it has been compiled. Try the code below to see:\n",
    "- The layers and their order in the model.\n",
    "- The output shape of each layer.\n",
    "- The number of parameters (weights) in each layer.\n",
    "- The total number of parameters (weights) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the y_values are categorical in nature (categories instead of numbers), we have to convert the y_values from categories into numbers before we can train the neural network. For neural networks, if the categories are not numerical groups (for example, 1,2,3,4,etc) we have to perform label encoding (Remember doing this in an earlier notebook?) before doing one-hot encoding. Refer to the bonus section in the \"Supervised learning technique\" notebook for more information on one-hot encoding. You can also read this [article](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) to find out more. We can use the to_categorical function from Keras to help us do so. Try the code below to label encode then one-hot encode the y_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: class, dtype: int64\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: class, dtype: int64\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Print number of data points in each class\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Dictionary to input the different numbers for different classes. \n",
    "# We are using values starting from 0 so that only 3 instead of 4 columns will be created in by the one-hot encoding.\n",
    "label_encode = {\"class\": {\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2}}\n",
    "\n",
    "# Use .replace to change the different classes into numbers\n",
    "df.replace(label_encode,inplace=True)\n",
    "\n",
    "# Print number of data points in each class to check if the classes have changed to numbers\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Extract out the class as y_values\n",
    "y_values = df['class']\n",
    "\n",
    "# One-hot encode the y_values\n",
    "y_values = to_categorical(y_values)\n",
    "\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what is listed in the y_values above. We have encoded the flower names into numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model. Run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0399 - accuracy: 0.5133\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 750us/step - loss: 1.0260 - accuracy: 0.5400\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.0111 - accuracy: 0.5533\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9955 - accuracy: 0.5533\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9805 - accuracy: 0.5667\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 750us/step - loss: 0.9658 - accuracy: 0.5800\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9510 - accuracy: 0.5800\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9374 - accuracy: 0.6067\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 0.9234 - accuracy: 0.6133\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9110 - accuracy: 0.6267\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8976 - accuracy: 0.6333\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8853 - accuracy: 0.6400\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 999us/step - loss: 0.8737 - accuracy: 0.6467\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8620 - accuracy: 0.6533\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8513 - accuracy: 0.6533\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 0.8407 - accuracy: 0.6600\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 999us/step - loss: 0.8311 - accuracy: 0.6600\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 0.8215 - accuracy: 0.6733\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8125 - accuracy: 0.6800\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.8039 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb1896fe48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have trained your first neural network. Before we look at the accuracy, we have to understand some of the terms in the output.\n",
    "\n",
    "Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "\n",
    "us/step shows how long the model took to train on each epoch.\n",
    "\n",
    "acc shows how accurate the model is. \n",
    "\n",
    "Notice how these numbers change over the different epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model above, what was the accuracy value you've got?\n",
    "\n",
    "Try to see if you can get a better accuracy by adding another hidden layer to the model. The additional hidden layer can have the same number of nodes as the previous layer. \n",
    "\n",
    "Copy the relevant codes listed above in the cell below and modify the codes to add in the additional hidden layer. Train the new model and see if the accuracy improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding another hidden layer, you should observe that the accuracy seems to be lower than the initial model. As such, this shows that adding more layers need not necessarily lead to a higher accuracy.\n",
    "\n",
    "<font color = blue>Bonus: You can try other methods to improve the accuracy. How about increasing the number of nodes in the hidden layer? Do you get a higher accuracy if you do so?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that the neural network can be trained for any number of epochs. This means that the network can keep learning from the same dataset many times. What do you think will happen if the model keeps learning from the same dataset? Do you think the network will be able to obtain very high accuracy by doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You were right if you think that the model's accuracy will increase as it continues to learn. You can try it on the same dataset. Run the code below and observe the accuracy. Is it higher than the accuracy from the previous model with the same setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the neural network as model\n",
    "model4 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or input_layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model4.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model4.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Train model with x_values and y_values. Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "model4.fit(x_values,y_values,epochs=200,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe from the printout that the accuracy score is now above 90%. Wow! It seems amazing that just increasing the number of epochs will allow the accuracy to increase. Do you think it is good that the model is so highly accuracte on the data that it has trained on? Just imagine, will a soccer player do well in a match if he/she only trains extremely hard on scoring a goal from a specific spot on the field? Or will a baker be able to bake a delicious cake based on a customer's request if he/she only learns how to bake a cake that is a specific flavour, shape and size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soccer player will not perform well in a match as he/she may not be able to shoot accurately from other parts of the field. The baker will not be able to bake a delicious cake as he/she will only know 1 specific flavour.\n",
    "\n",
    "The idea behind this question is the concept of overfitting. If the model overfits, it will not be able to generalise to properly predict data that it has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the concept of overfitting and also applies to all machine learning techniques. Once the technique has fitted too acurrately on the dataset, the trained technique will not be able to generalise to other data that it has not seen before. As such, we usually only train the technique on a fraction of the dataset that we have and keep the remainder as a test or validation set to see if the model has overfitted. When training on the training set, the accuracy of the model on the test set should increase. However, at the point of overfitting, the accuracy on the test set will start to decrease. If you see that the test accuracy has begin to increase after a certain epoch, you should not train the model any further.\n",
    "\n",
    "Let us apply the split between a train and a test set to the dataset that we are using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to decide how much data we want to keep and prevent the model from training on. Usually, we withold about 20% to 30% of the dataset. In this example, we will keep 25 percent of the dataset as the test/validation set. We can use the train_test_split function from the sklearn library. Try the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract out original x_values from the dataframe df. \n",
    "# We have to re-extract the x_values as the standardisation should only be based on the data that the model will train with.\n",
    "# Thus, we have to split the data first before standardising.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.25 indicates that 25% of the datapoints will be in x_test and y_test whereas 75% will be in x_train and y_train.\n",
    "# random_state=10 is used to ensure that the split is the same everytime you run the code below. \n",
    "# This is because the split is done randomly everytime. The same random_state is the only way to ensure the same split everytime.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n",
    "\n",
    "# Check the number of rows in x_train, x_test, y_train and y_test\n",
    "print(\"Number of rows in x_train:\", x_train.shape[0])\n",
    "print(\"Number of rows in x_test:\", x_test.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# We can now standardise the x values.\n",
    "# Initialise the StandardScaler\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# Standardise the x_train values using .fit_transform\n",
    "x_train = standardise.fit_transform(x_train)\n",
    "\n",
    "# Standardise the x_test values using .transform. \n",
    "# There is no need to fit the data as the standardisation should be the same as that of x_train.\n",
    "x_test = standardise.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own code to split the dataset into train and test/validation sets by witholding 20% of the dataset as test/validation set. Use x_train2, x_test2, y_train2 and y_test2 as your variables. Check that the number of rows are correct for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use x_train2, x_test2, y_train2 and y_test2 to train a neural network. Write a code to create a neural network that has 2 hidden layers with 6 nodes each and 1 output layer with 3 nodes. The activation for the input hidden layer is 'relu' wheras the activation for the output layer is 'softmax'. The opitmizer to use is 'adam' and the loss should be 'categorical_crossentropy'. The metrics should be 'accuracy'. Use model_val as the model variable. Print the model summary after you compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train model_val using x_train and y_train. We will also test the accuracy after each epoch using x_test and y_test. Try the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "# Validation_data will allow us to input in the test/validation datasets. This will allow us to see the model accuracy on the test/validation set.\n",
    "model_val.fit(x_train,y_train,epochs=50,shuffle=True,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the printout now shows the validation accuracy as well? Is the validation accuracy higher or lower than the training accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the model to identify flower types for newly gathered data. For example, imagine that your friend has measured the sepal_length, sepal_width, petal_length and petal_width of some flowers and saved the data in a file called \"iris_predict.data\". Your friend wants to find the flower types for these flowers based on the values measured. Would you be able to use your model to help your friend? What are the flower types that your friend measured? Try the codes below!\n",
    "<font color=blue>Hint: You can use model.predict method to obtain the flower types for your friend. Additionally, the values returned by the .predict method will be the probability of each flower type that the model thinks should be assigned to each data row. As such, the higher the probablity, the more confident the model is that the flower type predicted is correct. For example, if the predicted values are very high in the second column, then the model thinks that the flower type is versicolor. You will also need to scale your data before obtaining the flower types. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./iris_predict.data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\"]\n",
    "df2.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = df2[['sepal_length','sepal_width','petal_length','petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_scale = standardise.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = model_val.predict(x_new_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the flower types for each data row? You can use the [argmax](https://www.geeksforgeeks.org/numpy-argmax-python/) function to help you identify the flower types from y_new. Try the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_types = []\n",
    "for ii in range(0,y_new.shape[0]):\n",
    "    flower_types.append(np.argmax(y_new[ii,:]))\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to match the flower type numbers back to the different types (Setosa, Versicolor, Virginica)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have successfully learnt how to create an artificial neural network model and trained it. Artficial neural networks are extremely powerful tools that can be used on very large datasets. For example, if you have a couple of hundred columns/features and over 100000 data points, it may be beneficial to use artificial neural networks instead of other machine learning techniques. Just remember, there are no strict rules on the number of nodes in the hidden layer or the number of hidden layers. Experiment with different neural networks to see which one provides the best result for your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITE",
   "language": "python",
   "name": "ite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
